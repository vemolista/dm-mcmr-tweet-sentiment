{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\vemol\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\vemol\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def get_percentage_of_total(item, total):\n",
    "    return item / total * 100\n",
    "\n",
    "def get_tweets_text_from_file(file_name):\n",
    "    #file = open(file_name, newline='') as csvfile\n",
    "    tweets_text = []\n",
    "\n",
    "    with open(file_name, \"r\", newline=\"\", encoding=\"utf=16\") as csvfile:\n",
    "        reader = csv.reader(csvfile, quotechar=\"|\")\n",
    "        for row in csvfile:\n",
    "            row = row.replace(\"\\r\\n\", \"\")\n",
    "            tweets_text.append(row)\n",
    "    return tweets_text\n",
    "\n",
    "def clean_text_array(text_array):\n",
    "    clean_text_array = []\n",
    "    for text in text_array:\n",
    "        # clean urls - RegEx found somewhere on StackOverflow\n",
    "        text = re.sub('(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\\s?', \"\", text)\n",
    "        # clean punctuation\n",
    "        text = \"\".join([c for c in text if c not in string.punctuation])\n",
    "        clean_text_array.append(text)\n",
    "    return clean_text_array\n",
    "\n",
    "def tokenize_text_array(text_array):\n",
    "    eng_stopwords = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    tokenized_text_array = []\n",
    "    tokens = []\n",
    "\n",
    "    for text in text_array:\n",
    "        tokens = TweetTokenizer(strip_handles=True, reduce_len=True).tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if word not in eng_stopwords and len(word) > 2 and word.isalpha()]\n",
    "        #tokens = [stemmer.stem(word) for word in tokens]\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "        tokenized_text_array.append(tokens)\n",
    "    \n",
    "    return tokenized_text_array\n",
    "\n",
    "def sentiment_analysis(token_array):\n",
    "    positive = 0\n",
    "    neutral = 0\n",
    "    negative = 0\n",
    "\n",
    "    for tweet in token_array:\n",
    "        analysis = TextBlob(\" \".join(tweet))\n",
    "        sentiment = analysis.sentiment.polarity\n",
    "\n",
    "        if sentiment > 0:\n",
    "            positive += 1\n",
    "        elif sentiment == 0:\n",
    "            neutral += 1\n",
    "        else:\n",
    "            negative += 1\n",
    "\n",
    "    total_analyzed = positive + neutral + negative\n",
    "    positive_percentage = get_percentage_of_total(positive, total_analyzed)\n",
    "    negative_percentage = get_percentage_of_total(negative, total_analyzed)\n",
    "    neutral_percentage = get_percentage_of_total(neutral, total_analyzed)\n",
    "\n",
    "    result = [positive, negative, neutral, positive_percentage, negative_percentage, neutral_percentage, total_analyzed]\n",
    "    return result\n",
    "\n",
    "def write_sentiment_results_to_csv(output_file_name, results_array, file_names):\n",
    "    #write to csv\n",
    "    file = open(output_file_name, \"w\", newline='', encoding=\"utf-16\")\n",
    "    with file:\n",
    "        field_names = ['result for', 'positive count', 'positive percentage', 'negative count', 'negative percentage', 'neutral count', 'neutral percentage', 'total']\n",
    "\n",
    "        writer = csv.DictWriter(file, fieldnames=field_names)\n",
    "        writer.writeheader()\n",
    "        for i in range(len(results_array)):\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    'result for': file_names[i], \n",
    "                    'positive count': results_array[i][0], \n",
    "                    'positive percentage': round(results_array[i][3], 2),\n",
    "                    'negative count': results_array[i][1],\n",
    "                    'negative percentage': round(results_array[i][4], 2),\n",
    "                    'neutral count': results_array[i][2],\n",
    "                    'neutral percentage': round(results_array[i][5], 2),\n",
    "                    'total': results_array[i][6]\n",
    "                })\n",
    "\n",
    "def analyze(file_name):\n",
    "    tweet_array = get_tweets_text_from_file(file_name)\n",
    "    cleaned_tweet_array = clean_text_array(tweet_array)\n",
    "    cleaned_tokenized_tweet_array = tokenize_text_array(cleaned_tweet_array)\n",
    "    sentiment_result = sentiment_analysis(cleaned_tokenized_tweet_array)\n",
    "\n",
    "    print(\"{} analyzed\".format(file_name))\n",
    "    return sentiment_result\n",
    "\n",
    "def do_it_all(file_names):\n",
    "    sentiment_results = []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        result = analyze(file_name)\n",
    "        sentiment_results.append(result)\n",
    "    \n",
    "    write_sentiment_results_to_csv(\"results.csv\", sentiment_results, file_names)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2017-03-28-pepsi.csv analyzed\n2017-03-29-pepsi.csv analyzed\n2017-03-30-pepsi.csv analyzed\n2017-03-31-pepsi.csv analyzed\n2017-04-01-pepsi.csv analyzed\n2017-04-02-pepsi.csv analyzed\n2017-04-03-pepsi.csv analyzed\n2017-04-04-pepsi.csv analyzed\n2017-04-05-pepsi.csv analyzed\n2017-04-06-pepsi.csv analyzed\n2017-04-07-pepsi.csv analyzed\n2017-04-08-pepsi.csv analyzed\n2017-04-09-pepsi.csv analyzed\n2017-04-10-pepsi.csv analyzed\n2017-04-11-pepsi.csv analyzed\n2017-04-12-pepsi.csv analyzed\n2017-04-13-pepsi.csv analyzed\ndone\n"
    }
   ],
   "source": [
    "file_names = [\"2017-03-28-pepsi.csv\", \"2017-03-29-pepsi.csv\", \"2017-03-30-pepsi.csv\", \"2017-03-31-pepsi.csv\", \"2017-04-01-pepsi.csv\", \"2017-04-02-pepsi.csv\", \"2017-04-03-pepsi.csv\", \"2017-04-04-pepsi.csv\", \"2017-04-05-pepsi.csv\", \"2017-04-06-pepsi.csv\", \"2017-04-07-pepsi.csv\", \"2017-04-08-pepsi.csv\", \"2017-04-09-pepsi.csv\", \"2017-04-10-pepsi.csv\", \"2017-04-11-pepsi.csv\", \"2017-04-12-pepsi.csv\", \"2017-04-13-pepsi.csv\"]\n",
    "\n",
    "do_it_all(file_names)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}